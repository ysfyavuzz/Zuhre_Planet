# ğŸš€ Mistral 7B Local Model Setup Guide

## NSFW KÄ±sÄ±tlamasÄ± Olmayan, Local AI Model Kurulumu

### Sistem Ã–zellikleri (Sizin)
```
âœ… 8 CPU Cores
âœ… 16 GB RAM
âœ… 157 GB Disk
âœ… Intel UHD Graphics 620
```

### SeÃ§ilen Model: Mistral 7B
```
âœ… AÃ§Ä±k kaynak
âœ… NSFW kÄ±sÄ±tlamasÄ± YOK
âœ… 16GB RAM'e mÃ¼kemmel
âœ… YÃ¼ksek kalite output
âœ… HÄ±zlÄ± iÅŸleme
```

---

## ğŸ“¥ AdÄ±m 1: Ollama Kur

### Windows
1. https://ollama.ai/download git
2. "Download for Windows" tÄ±kla
3. `.exe` dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±r
4. Kurulumu tamamla

### Kurulumu DoÄŸrula
```bash
ollama --version
```

---

## ğŸ“¦ AdÄ±m 2: Mistral 7B Model Ä°ndir

Ollama baÅŸladÄ±ktan sonra terminal'de:

```bash
ollama pull mistral
```

**BurasÄ± zaman alacak** (~4.1GB download, ~10-15 dakika)

Ä°ndir tamamlanÄ±rsa:
```
Success! Pulled mistral:latest
```

---

## âœ… AdÄ±m 3: Model'i Test Et

```bash
ollama run mistral
```

Terminal'de:
```
>>> Merhaba, sen kimsin?
I am Mistral, an AI assistant...

>>> KullanÄ±cÄ± profili sayfasÄ± nasÄ±l yapabilirim?
Here's a complete example...

>>> exit
```

---

## ğŸ”Œ AdÄ±m 4: CLI'ye Entegre Et

Zuhre Planet CLI'yi Mistral ile kullan:

### A. Local API Server BaÅŸlat

Terminal 1'de:
```bash
ollama serve
```

Ã‡Ä±ktÄ±:
```
Listening on 127.0.0.1:11434
```

### B. CLI'yi GÃ¼ncelle

`cli/zuhre-cli.ts` dosyasÄ±nda, Anthropic client'i yerine Ollama kullan:

```typescript
// Ollama endpoint
const OLLAMA_API = process.env.OLLAMA_API || "http://127.0.0.1:11434";
```

### C. Yeni CLI Komut Ekle

Terminal 2'de:
```bash
npm run zuhre -- local-chat
```

---

## ğŸ¯ KullanÄ±m

### Terminal 1: Model Ã‡alÄ±ÅŸtÄ±r
```bash
ollama serve
```

### Terminal 2: CLI Kullan
```bash
# Chat mode
npm run zuhre -- chat

# Kod analizi
npm run zuhre -- analyze

# Feature geliÅŸtirme
npm run zuhre -- feature "yeni Ã¶zellik"
```

---

## ğŸ“Š Alternatif Modeller

### Llama 2 7B
```bash
ollama pull llama2
```

### Neural Chat 7B
```bash
ollama pull neural-chat
```

### OpenHermes 2.5
```bash
ollama pull openhermes
```

### TÃ¼m Modelleri GÃ¶rmek
```bash
ollama list
```

---

## âš¡ Performans Optimizasyonu

### Ollama Context Size ArttÄ±r
```bash
# Model'i 8192 token context ile Ã§alÄ±ÅŸtÄ±r
ollama run mistral --context-size 8192
```

### GPU Kullan (EÄŸer NVIDIA varsa)
```bash
# CUDA desteÄŸi ile
# Ollama otomatik CUDA bulur
```

### RAM YÃ¶netimi
```bash
# Arka planda birden fazla model Ã§alÄ±ÅŸtÄ±rmayÄ±n
# Ollama otomatik unload eder eski modelleri
```

---

## ğŸ“ Docker ile Ollama

Alternatif: Docker container'da Ã§alÄ±ÅŸtÄ±r

```bash
docker run -d -v ollama:/root/.ollama -p 11434:11434 ollama/ollama
```

Sonra model Ã§ek:
```bash
docker exec <container-id> ollama pull mistral
```

---

## ğŸ”— CLI Integration Script

`cli/ollama-setup.ts` oluÅŸtur:

```typescript
import fetch from 'node-fetch';

const OLLAMA_API = "http://127.0.0.1:11434/api/generate";

export async function askMistral(prompt: string): Promise<string> {
  const response = await fetch(OLLAMA_API, {
    method: "POST",
    body: JSON.stringify({
      model: "mistral",
      prompt: prompt,
      stream: true,
    }),
  });

  let result = "";
  for await (const chunk of response.body) {
    const line = chunk.toString();
    if (line) {
      const json = JSON.parse(line);
      result += json.response;
      process.stdout.write(json.response);
    }
  }
  return result;
}

// KullanÄ±m:
// const answer = await askMistral("Escort profili sayfasÄ± nasÄ±l yapÄ±lÄ±r?");
```

---

## ğŸ“ Mistral 7B Ã–zellikleri

âœ… **Strengths:**
- NSFW kÄ±sÄ±tlamasÄ± YOK
- HÄ±zlÄ± inference
- DÃ¼ÅŸÃ¼k resource kullanÄ±mÄ±
- Ä°yi code generation
- Turkish dil desteÄŸi

âš ï¸ **Limitations:**
- Llama 2 13B kadar detaylÄ± deÄŸil
- Bazen repetitif olabilir
- Context window 8192 token

---

## ğŸ“Š Beklenen Performans

| GÃ¶rev | HÄ±z | Kalite |
|-------|-----|--------|
| Code generation | âš¡âš¡âš¡ Ã‡ok hÄ±zlÄ± | â­â­â­â­ Ä°yi |
| Q&A | âš¡âš¡âš¡ Ã‡ok hÄ±zlÄ± | â­â­â­â­ Ä°yi |
| Summarization | âš¡âš¡âš¡ Ã‡ok hÄ±zlÄ± | â­â­â­â­ Ä°yi |
| Creative writing | âš¡âš¡ HÄ±zlÄ± | â­â­â­â­ Ä°yi |
| Analysis | âš¡âš¡ HÄ±zlÄ± | â­â­â­ Orta |

---

## ğŸš€ Tavsiye Edilen Workflow

### 1. Ollama BaÅŸlat
```bash
ollama serve
```

### 2. Model Ã‡alÄ±ÅŸtÄ±r
```bash
ollama run mistral
```

### 3. CLI ile Kullan
```bash
npm run zuhre -- local-chat
```

### 4. Requests YapÄ±ÅŸtÄ±r
```
>>> Zuhre Planet'e yeni 3D avatar sistemi nasÄ±l ekleyebilirim?
```

---

## ğŸ’¡ NSFW Content Handling

Mistral 7B'nin NSFW kÄ±sÄ±tlamasÄ± yoktur:

```
>>> Adult website iÃ§in uygun prompts yazabilir misin?
(Mistral hiÃ§ bir kÄ±sÄ±tlama yapmadan cevaplar)
```

Bunu kontrol etmek isterseniz:
```bash
# Moderasyon ekle (opsiyonel)
# Openai Moderation API yerine local model kullan
```

---

## ğŸ“š Kaynaklar

- **Ollama**: https://ollama.ai
- **Mistral**: https://mistral.ai
- **Model Docs**: https://docs.ollama.ai

---

## âœ… Kontrol Listesi

- [ ] Ollama indir ve kur
- [ ] `ollama pull mistral` Ã§alÄ±ÅŸtÄ±r
- [ ] `ollama serve` baÅŸlat (Terminal 1)
- [ ] `ollama run mistral` test et
- [ ] CLI'ye entegre et
- [ ] `npm run zuhre -- chat` kullan

---

## ğŸ‰ HazÄ±r!

ArtÄ±k NSFW kÄ±sÄ±tlamasÄ± olmayan, kendi bilgisayarÄ±nÄ±zda Ã§alÄ±ÅŸan AI modeli var!

```bash
ollama serve
# BaÅŸka bir terminal'de:
npm run zuhre -- chat
```

Sorular sor, code oluÅŸtur, adult web sitesi geliÅŸtir! ğŸš€

---

**Herhangi bir soru varsa haber ver!**
